from django.shortcuts import render
import os
import torch
import json
import whisper  # âœ… Whisper ì¶”ê°€
from pydub import AudioSegment
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt
from django.views import View
from django.utils.decorators import method_decorator
from transformers import BertTokenizer

# KoBERT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
from kobert.pytorch_kobert import get_pytorch_kobert_model

# FFmpeg ê²½ë¡œ ì§ì ‘ ì„¤ì •
FFMPEG_PATH = r"D:\ffmpeg-master-latest-win64-gpl-shared\ffmpeg-master-latest-win64-gpl-shared\bin\ffmpeg.exe"
AudioSegment.converter = FFMPEG_PATH
print(f"ğŸ”§ FFmpeg ì„¤ì • ì™„ë£Œ: {FFMPEG_PATH}")

# Whisper ëª¨ë¸ ë¡œë“œ
whisper_model = whisper.load_model("base")

# ì„ì‹œ íŒŒì¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
TEMP_DIR = os.path.join(os.getcwd(), "temp_files")
os.makedirs(TEMP_DIR, exist_ok=True)  # âœ… í´ë” ì—†ìœ¼ë©´ ìƒì„±

class BERTClassifier(torch.nn.Module):
    def __init__(self):
        super(BERTClassifier, self).__init__()
        self.bert, _ = get_pytorch_kobert_model()
        self.classifier = torch.nn.Linear(768, 1)

    def forward(self, token_ids, valid_length, segment_ids):
        _, pooled_output = self.bert(input_ids=token_ids, return_dict=False)
        return self.classifier(pooled_output)

# KoBERT ëª¨ë¸ ì´ˆê¸°í™”
MODEL_PATH = os.path.join(os.path.dirname(__file__), "kobert_state_dict.pth")
model = BERTClassifier()
model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device("cpu")))
model.eval()
tokenizer = BertTokenizer.from_pretrained("monologg/kobert")


@method_decorator(csrf_exempt, name="dispatch")
class AudioFileUploadView(View):
    def post(self, request):
        """ ìŒì„± íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë³€í™˜ í›„ ë¶„ì„ """
        print("[ğŸ“Œ ìš”ì²­] íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„ ìš”ì²­")

        if "file" not in request.FILES:
            return JsonResponse({"error": "íŒŒì¼ì´ í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}, status=400)

        uploaded_file = request.FILES["file"]
        ext = os.path.splitext(uploaded_file.name)[1].lower()
        allowed_extensions = [".mp3", ".wav", ".ogg", ".m4a"]

        if ext not in allowed_extensions:
            return JsonResponse({"error": "ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."}, status=400)

        try:
            # ğŸ”¹ íŒŒì¼ì„ TEMP_DIRì— ì €ì¥
            temp_audio_path = os.path.join(TEMP_DIR, uploaded_file.name)
            with open(temp_audio_path, "wb") as f:
                f.write(uploaded_file.read())

            # ğŸ”¹ ë³€í™˜: mp3, ogg â†’ wav
            wav_file_path = os.path.join(TEMP_DIR, os.path.splitext(uploaded_file.name)[0] + ".wav")
            print(f"[ğŸ™ï¸ ë³€í™˜] {ext} â†’ WAV ë³€í™˜ ì¤‘...")

            try:
                audio = AudioSegment.from_file(temp_audio_path, format=ext[1:])
                audio.export(wav_file_path, format="wav")
            except Exception as e:
                print(f"ğŸš¨ [ì˜¤ë¥˜] FFmpeg ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
                return JsonResponse({"error": "FFmpeg ë³€í™˜ ì‹¤íŒ¨"}, status=500)

            # ğŸ”¹ ë³€í™˜ í›„ íŒŒì¼ í™•ì¸
            if not os.path.exists(wav_file_path):
                print(f"ğŸš¨ [ì˜¤ë¥˜] WAV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {wav_file_path}")
                return JsonResponse({"error": "WAV ë³€í™˜ í›„ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."}, status=500)

            print(f"[ğŸ™ï¸ STT ì‹œì‘]: {wav_file_path}")

            # ğŸ”¹ STT ì‹¤í–‰
            text = AudioFileUploadView.audio_to_text(wav_file_path)

            if text.startswith("Whisper ë³€í™˜ ì‹¤íŒ¨"):
                return JsonResponse({"error": text}, status=500)

            print(f"[ğŸ” ë¶„ì„í•  í…ìŠ¤íŠ¸]: {text}")

            probability = self.analyze_text(text) * 100

            # ğŸ”¹ ì„ì‹œ íŒŒì¼ ì‚­ì œ
            # os.remove(temp_audio_path)
            # os.remove(wav_file_path)

            return JsonResponse({"probability": probability, "text": text}, status=200)

        except Exception as e:
            print(f"ğŸš¨ [ERROR] ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return JsonResponse({"error": "íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}, status=500)

    @staticmethod
    def audio_to_text(wav_file_path):
        """ ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ """
        if not os.path.exists(wav_file_path):
            print(f"ğŸš¨ [ì˜¤ë¥˜] íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {wav_file_path}")
            return "Whisper ë³€í™˜ ì‹¤íŒ¨: íŒŒì¼ ì—†ìŒ"

        print(f"[ğŸ™ï¸ STT ì‹œì‘]: {wav_file_path}")
        try:
            result = whisper_model.transcribe(wav_file_path)
            print(result)
            return result["text"]
        except Exception as e:
            print(f"ğŸš¨ [ì˜¤ë¥˜] Whisper ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
            return f"Whisper ë³€í™˜ ì‹¤íŒ¨: {str(e)}"

    def analyze_text(self, text):
        """ KoBERT ëª¨ë¸ì„ ì‚¬ìš©í•´ í…ìŠ¤íŠ¸ ë¶„ì„ """
        try:
            inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
            valid_length = torch.tensor([len(inputs["input_ids"][0])])  # ë¬¸ì¥ ê¸¸ì´
            segment_ids = torch.zeros_like(inputs["input_ids"])  # ëª¨ë“  ë‹¨ì–´ì˜ segment IDë¥¼ 0ìœ¼ë¡œ ì„¤ì •

            with torch.no_grad():
                output = model(inputs["input_ids"], valid_length, segment_ids)
            
            val = output.squeeze(1)
            chk = torch.sigmoid(val)
            chk = chk.item()

            return chk
        
        except Exception as e:
            print(f"ğŸš¨ [ERROR] ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return "ì˜¤ë¥˜ ë°œìƒ"
